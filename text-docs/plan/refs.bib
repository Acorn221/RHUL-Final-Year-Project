@article{LotteryTicket,
  author     = {Jonathan Frankle and
                Michael Carbin},
  title      = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal    = {CoRR},
  volume     = {abs/1803.03635},
  year       = {2018},
  url        = {http://arxiv.org/abs/1803.03635},
  eprinttype = {arXiv},
  eprint     = {1803.03635},
  timestamp  = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1803-03635.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}



@book{FFT,
  author = {James W. Cooley and John W. Tukey},
  title  = {An Algorithm for the Machine Calculation of Complex Fourier Series},
  year   = {1965}
}

@article{N2PS,
  author  = {Augasta, M.Gethsiyal and Kathirvalavakumar, T.},
  year    = {2011},
  month   = {12},
  pages   = {241-258},
  title   = {A Novel Pruning Algorithm for Optimizing Feedforward Neural Network of Classification Problems},
  volume  = {34},
  journal = {Neural Processing Letters},
  doi     = {10.1007/s11063-011-9196-7}
}

@inproceedings{EmbeddedSystems,
  author    = {Zhang, Zhongpeng},
  booktitle = {2021 2nd International Conference on Computing and Data Science (CDS)},
  title     = {Model Pruning Techniques for Boosting the Inference Efficiency on Embedded Systems},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {119-124},
  doi       = {10.1109/CDS52072.2021.00027}
}

@article{DBLP:journals/corr/abs-2106-06955,
  author     = {Jaron Maene and
                Mingxiao Li and
                Marie{-}Francine Moens},
  title      = {Towards Understanding Iterative Magnitude Pruning: Why Lottery Tickets
                Win},
  journal    = {CoRR},
  volume     = {abs/2106.06955},
  year       = {2021},
  url        = {https://arxiv.org/abs/2106.06955},
  eprinttype = {arXiv},
  eprint     = {2106.06955},
  timestamp  = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2106-06955.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2003-03033,
  author     = {Davis W. Blalock and
                Jose Javier Gonzalez Ortiz and
                Jonathan Frankle and
                John V. Guttag},
  title      = {What is the State of Neural Network Pruning?},
  journal    = {CoRR},
  volume     = {abs/2003.03033},
  year       = {2020},
  url        = {https://arxiv.org/abs/2003.03033},
  eprinttype = {arXiv},
  eprint     = {2003.03033},
  timestamp  = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-03033.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BabaeizadehSC16,
  author     = {Mohammad Babaeizadeh and
                Paris Smaragdis and
                Roy H. Campbell},
  title      = {NoiseOut: {A} Simple Way to Prune Neural Networks},
  journal    = {CoRR},
  volume     = {abs/1611.06211},
  year       = {2016},
  url        = {http://arxiv.org/abs/1611.06211},
  eprinttype = {arXiv},
  eprint     = {1611.06211},
  timestamp  = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/BabaeizadehSC16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2202-03844,
  author     = {Javier Poyatos and
                Daniel Molina and
                Aritz D. Martinez and
                Javier Del Ser and
                Francisco Herrera},
  title      = {EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning
                based Deep Neural Networks},
  journal    = {CoRR},
  volume     = {abs/2202.03844},
  year       = {2022},
  url        = {https://arxiv.org/abs/2202.03844},
  eprinttype = {arXiv},
  eprint     = {2202.03844},
  timestamp  = {Wed, 09 Feb 2022 15:43:35 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2202-03844.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{gupta2022global,
  title  = {Global Magnitude Pruning With Minimum Threshold Is All We Need},
  author = {Manas Gupta and Vishandi Rudy Keneta and Abhishek Vaidyanathan and Ritwik Kanodia and Efe Camci and Chuan-Sheng Foo and Jie Lin},
  year   = {2022},
  url    = {https://openreview.net/forum?id=jNB6vfl_680}
}

@misc{ShrinkBench,
  title = {Open source PyTorch library to facilitate development and standardized evaluation of neural network pruning methods.},
  url   = {https://github.com/JJGO/shrinkbench}
}

@misc{Keras,
  title = {Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.},
  url   = {https://keras.io/}
}

@misc{PyTorch,
  title = {PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR).},
  url   = {https://pytorch.org/}
}

@misc{TensorFlow,
  title = {TensorFlow is an end-to-end open source platform for machine learning.},
  url   = {https://www.tensorflow.org/}
}

@misc{Jupyter,
  title = {JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data.},
  url   = {https://jupyter.org/}
}

@misc{Matplotlib,
  title = {Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.},
  url   = {https://matplotlib.org/}
}

@misc{Pandas,
  title = {Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.},
  url   = {https://pandas.pydata.org/}
}

@misc{RHULGitLab,
	title = {RHUL GitLab},
	url   = {https://gitlab.cim.rhul.ac.uk/}
}

@misc{GTX1070,
	title = {NVIDIA GeForce GTX 1070},
	url   = {https://www.nvidia.com/en-gb/geforce/products/10series/geforce-gtx-1070/}
}

@misc{ImageNet,
	title = {ImageNet},
	url   = {http://www.image-net.org/}
}

@misc{Trello,
	title = {Trello},
	url   = {https://trello.com/}
}